%
In recent years there has been significant increase in use of word embedding methods for use in NLP applications. Amongst those methods neural network based methods are the most popular ones. Although these methods are very popular these days we believe that still SVD methods still are quiet interesting methods to be investigated.  In theory, it has been shown that method like Skip Gram with Negative Sampling (SGNS) is equivalent to performing SVD on shifted PPMI matrix. Although in English language SVD based methods has underperformed compared to SGNS methods. In this project we are going to elaborate same thing in Persian language. The main ideas in this project are based on works done by \cite{DBLP:conf/conll/2014} \cite{DBLP:journals/tacl/LevyGD15} \cite{NIPS2014_5477}. 

One of the best achievements of this project was to have an end-to-end embedding method that we all understand and can debug and work with it. Unfortunately most of the times that we use embedding method is through ready made tools and this create a black box for us in embedding. Most of pre defined tools have lots of side logics that is hard to capture  or alter. As result of this project we now have a white box that we have control and understanding of it.  

